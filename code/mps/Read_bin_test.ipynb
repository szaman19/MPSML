{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "import os \n",
    "import os.path as osp \n",
    "import numpy as np \n",
    "from tqdm import tqdm\n",
    "\n",
    "with open(\"7-qubits.bin\", mode='rb') as file: # b is important -> binary\n",
    "    fileContent = file.read()\n",
    "print(fileContent)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1580.3616\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "iterative unpacking requires a buffer of a multiple of 1200 bytes",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-bc9d7963d0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"7-qubits.bin\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstruct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_unpack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_specificer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31merror\u001b[0m: iterative unpacking requires a buffer of a multiple of 1200 bytes"
     ]
    }
   ],
   "source": [
    "num_qubits = 7\n",
    "num_fields = 3 * num_qubits\n",
    "num_energies = 1\n",
    "num_coefficients = 2 ** (num_qubits)\n",
    "\n",
    "num_doubles = num_fields + num_energies + num_coefficients\n",
    "struct_size = (num_fields + num_energies + num_coefficients) * 8 \n",
    "format_specificer = \"\".join(['d' for i in range(num_doubles)])\n",
    "format_specificer = \"@\"+format_specificer\n",
    "data = fileContent\n",
    "\n",
    "print(os.stat(\"7-qubits.bin\").st_size / 2500)\n",
    "data = struct.iter_unpack(format_specificer, data)\n",
    "print(data)\n",
    "\n",
    "\n",
    "\n",
    "# print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for learning_set in range (stop):\n",
    "\n",
    "    training_data_2 = auto_encoder.get_dataset_active(data_2,2,pool_size, pts)\n",
    "    training_data_4 = auto_encoder.get_dataset_active(data_4,4,pool_size, pts)\n",
    "    training_data_8 = auto_encoder.get_dataset_active(data_8,8,pool_size, pts) \n",
    "\n",
    "\n",
    "    split1 = int(len(pts)*0.9)\n",
    "    split2 = int(len(pts)/10)\n",
    "    split1 += len(pts)-(split1+split2)\n",
    "    \n",
    "    training_data_2, val_data_2 = random_split(training_data_2, [split1,split2])\n",
    "    training_data_4, val_data_4 = random_split(training_data_4, [split1,split2])\n",
    "    training_data_8, val_data_8 = random_split(training_data_8, [split1,split2])\n",
    "\n",
    "    datasets = [training_data_2,\n",
    "                training_data_4,\n",
    "                training_data_8]\n",
    "\n",
    "    training_loaders = [DataLoader(x, batch_size = 32,  shuffle=True, num_workers=20) for x in datasets]\n",
    "\n",
    "    val_data_6 = get_dataset(data_6, 6, pool_size)\n",
    "#     val_data_8 = get_dataset(data_8, 8, pool_size)\n",
    "#     val_data_9 = get_dataset(data_9, 9, pool_size)\n",
    "\n",
    "    val_datasets = [val_data_6, val_data_2, val_data_4, val_data_8] # val_data_8, val_data_9\n",
    "\n",
    "    val_loaders = [DataLoader(x, batch_size = 10, num_workers=20) for x in val_datasets]\n",
    "\n",
    "    warmup_2 = next(iter(training_loaders[0]))\n",
    "    warmup_4 = next(iter(training_loaders[1]))\n",
    "    warmup_8 = next(iter(training_loaders[2]))\n",
    "\n",
    "    mps_size = 5\n",
    "    model = auto_encoder.MPS_autoencoder(mps_size = mps_size)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    loss_func = nn.MSELoss(reduction='sum')\n",
    "\n",
    "    warmup_data = [(warmup_2,2), (warmup_4,4), (warmup_8,8)]\n",
    "\n",
    "    print(\"WARMUP TRAINING ITERATION: \", learning_set)\n",
    "    for j in range(10):\n",
    "        for i in range(3):\n",
    "            for epoch in range(10):\n",
    "                fields,wf = warmup_data[i][0]\n",
    "                gs = model(fields, warmup_data[i][1])            \n",
    "                loss = loss_func(gs, wf)\n",
    "                if (epoch % 10 == 0):\n",
    "                    current_loss = loss.item() *(2**warmup_data[i][1])\n",
    "                    print(warmup_data[i][1],\"\\t\", current_loss)\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    print(\"__________________________________________________\")\n",
    "    print()    \n",
    "    \n",
    "    print(\"Training Validation \", learning_set)\n",
    "    train_sizes, train_loader = enumerate(training_loaders)\n",
    "    val_6 = next(iter(val_loaders[0]))\n",
    "    #val_8 = next(iter(val_loaders[1]))\n",
    "    #val_9 = next(iter(val_loaders[2]))\n",
    "    val_2 = next(iter(val_loaders[1]))\n",
    "    val_4 = next(iter(val_loaders[2]))\n",
    "    val_8 = next(iter(val_loaders[3]))\n",
    "    \n",
    "    val_data = [(val_6,6),(val_2,2),(val_4,4),(val_8,8)]\n",
    "    val_loaders, val_sizes = val_data\n",
    "    \n",
    "    model, tot_err, val_err, t_errs, val_errsauto_encoder.mps_fit(mps_size, train_loaders, train_sizes, val_loaders, val_sizes)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_dataset = auto_encoder.get_dataset\n",
    "data_2 = '2_qubit_crit_data.npz'\n",
    "data_4 = '4_qubit_crit_data.npz'\n",
    "data_6 = '6_qubit_crit_data.npz'\n",
    "#data_7 = '7_qubit_crit_data.npz'\n",
    "data_8 = '8_qubit_crit_data.npz'\n",
    "# data_9 = '9_qubit_crit_data.npz'\n",
    "\n",
    "training_n_sizes = [2,4,8]\n",
    "validation_n_sizes = [6]\n",
    "\n",
    "pool_size = 2500\n",
    "\n",
    "pts = seed(pool_size)\n",
    "\n",
    "stop = 1\n",
    "\n",
    "error_p = []\n",
    "\n",
    "magnetization_6 = []\n",
    "wave_func_6 = []"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
